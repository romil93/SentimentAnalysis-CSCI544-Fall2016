{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although later on we will use `sklearn.feature_extraction.text.CountVectorizer` to create a bag-of-words set of features, and this library directly accepts a file name, we need to pass instead a secuence of documents since our training file contains not just text but also sentiment tags (that we need to strip out).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib, os\n",
    "\n",
    "data_list= []\n",
    "data_list_test = []\n",
    "for root, dirs, files in os.walk(\"/Users/romilvasani/USC/CSCI544/Final Project/dataset/aclImdb/train/pos\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_open = open(os.path.join(root, file), \"r\")\n",
    "            file_content = file_open.read()\n",
    "            data_list.append([file_content,\"POSITIVE\"])\n",
    "            \n",
    "\n",
    "for root, dirs, files in os.walk(\"/Users/romilvasani/USC/CSCI544/Final Project/dataset/aclImdb/train/neg\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_open = open(os.path.join(root, file), \"r\")\n",
    "            file_content = file_open.read()\n",
    "            data_list.append([file_content,\"NEGATIVE\"])\n",
    "\n",
    "            \n",
    "for root, dirs, files in os.walk(\"/Users/romilvasani/USC/CSCI544/Final Project/dataset/aclImdb/test/pos\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_open = open(os.path.join(root, file), \"r\")\n",
    "            file_content = file_open.read()\n",
    "            data_list_test.append([file_content,\"POSITIVE\"])\n",
    "            \n",
    "\n",
    "for root, dirs, files in os.walk(\"/Users/romilvasani/USC/CSCI544/Final Project/dataset/aclImdb/test/neg\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_open = open(os.path.join(root, file), \"r\")\n",
    "            file_content = file_open.read()\n",
    "            data_list_test.append([file_content,\"NEGATIVE\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our files downloaded locally, we can load them into data frames for processing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data_df = pd.DataFrame(data_list_test, columns=[\"Text\",\"Sentiment\"])\n",
    "test_data_df = pd.DataFrame(data_list, columns=[\"Text\",\"Sentiment\"])\n",
    "\n",
    "# test_data_df = pd.read_csv(test_data_file_name, header=None, sep=\"|\", usecols=[2])\n",
    "# test_data_df.columns = [\"Text\"]\n",
    "# train_data_df = pd.read_csv(train_data_file_name, header=None, sep=\"|\", usecols=[2,3])\n",
    "# train_data_df.columns = [\"Text\",\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `header=0` indicates that the first line of the file contains column names, `delimiter=\\t` indicates that the fields are separated by tabs, and `quoting=3` tells Python to ignore doubled quotes, otherwise you may encounter errors trying to read the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the first few lines of the train data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Sentiment\n",
       "0  I went and saw this movie last night after bei...  POSITIVE\n",
       "1  Actor turned director Bill Paxton follows up h...  POSITIVE\n",
       "2  As a recreational golfer with some knowledge o...  POSITIVE\n",
       "3  I saw this film in a sneak preview, and it is ...  POSITIVE\n",
       "4  Bill Paxton has taken the true story of the 19...  POSITIVE"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Sentiment\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...  POSITIVE\n",
       "1  Homelessness (or Houselessness as George Carli...  POSITIVE\n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...  POSITIVE\n",
       "3  This is easily the most underrated film inn th...  POSITIVE\n",
       "4  This is not the typical Mel Brooks film. It wa...  POSITIVE"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how many labels do we have for each sentiment class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NEGATIVE    12500\n",
       "POSITIVE    12500\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's calculate the average number of words per sentence. We could do the following using a list comprehension with the number of words per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228.51516000000001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "np.mean([len(s.split(\" \")) for s in train_data_df.Text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a *corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) in the wonderful `scikit learn` Python library converts a collection of text documents to a matrix of token counts. This is just what we need to implement later on our bag-of-words linear classifier.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to init the vectorizer. We need to remove puntuations, lowercase, remove stop words, and stem words. All these steps can be directly performed by `CountVectorizer` if we pass the right parameter values. We can do as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer        \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#######\n",
    "# based on http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    # remove non letters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # stem\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "######## \n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = 'english',\n",
    "    max_features = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `fit_transform` does two functions: First, it fits the model and learns the vocabulary; second, it transforms our corpus data into feature vectors. The input to `fit_transform` should be a list of strings, so we concatenate train and test data as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_data_features = vectorizer.fit_transform(train_data_df.Text.tolist() + test_data_df.Text.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays are easy to work with, so convert the result to an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data_features_nd = corpus_data_features.toarray()\n",
    "corpus_data_features_nd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['act', 'actor', 'actual', 'ani', 'anoth', 'bad', 'becaus', 'befor', 'believ', 'best', 'better', 'br', 'cast', 'charact', 'come', 'comedi', 'day', 'did', 'didn', 'direct', 'director', 'doe', 'doesn', 'don', 'effect', 'end', 'enjoy', 'everi', 'fact', 'feel', 'film', 'funni', 'girl', 'good', 'got', 'great', 'guy', 'ha', 'hi', 'horror', 'just', 'kill', 'know', 'life', 'like', 'littl', 'live', 'look', 'lot', 'love', 'm', 'make', 'man', 'mani', 'minut', 'movi', 'music', 'new', 'noth', 'old', 'onli', 'origin', 'peopl', 'perform', 'play', 'plot', 'point', 'pretti', 'quit', 'real', 'realli', 'role', 's', 'say', 'scene', 'seen', 'set', 'someth', 'star', 'start', 'stori', 't', 'thi', 'thing', 'think', 'thought', 'time', 'tri', 'turn', 'use', 've', 'veri', 'wa', 'want', 'watch', 'way', 'whi', 'work', 'world', 'year']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the counts of each word in the vocabulary as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17494 act\n",
      "13599 actor\n",
      "10011 actual\n",
      "15048 ani\n",
      "8594 anoth\n",
      "18544 bad\n",
      "17715 becaus\n",
      "8525 befor\n",
      "7841 believ\n",
      "12630 best\n",
      "11459 better\n",
      "201954 br\n",
      "8762 cast\n",
      "28364 charact\n",
      "13260 come\n",
      "7430 comedi\n",
      "7850 day\n",
      "12624 did\n",
      "8768 didn\n",
      "7450 direct\n",
      "10050 director\n",
      "11673 doe\n",
      "8876 doesn\n",
      "17661 don\n",
      "7248 effect\n",
      "19100 end\n",
      "8490 enjoy\n",
      "7968 everi\n",
      "7358 fact\n",
      "10353 feel\n",
      "95890 film\n",
      "8768 funni\n",
      "7881 girl\n",
      "30195 good\n",
      "7234 got\n",
      "18404 great\n",
      "9257 guy\n",
      "33400 ha\n",
      "57725 hi\n",
      "7507 horror\n",
      "35186 just\n",
      "7419 kill\n",
      "15170 know\n",
      "12938 life\n",
      "45210 like\n",
      "12435 littl\n",
      "8718 live\n",
      "19951 look\n",
      "9723 lot\n",
      "18221 love\n",
      "10176 m\n",
      "30035 make\n",
      "11943 man\n",
      "13486 mani\n",
      "7441 minut\n",
      "103284 movi\n",
      "8587 music\n",
      "8102 new\n",
      "8396 noth\n",
      "8811 old\n",
      "23241 onli\n",
      "7630 origin\n",
      "18384 peopl\n",
      "10733 perform\n",
      "17374 play\n",
      "13809 plot\n",
      "8068 point\n",
      "7258 pretti\n",
      "7498 quit\n",
      "9442 real\n",
      "23095 realli\n",
      "8386 role\n",
      "129794 s\n",
      "14992 say\n",
      "21452 scene\n",
      "13378 seen\n",
      "8033 set\n",
      "10219 someth\n",
      "8450 star\n",
      "8145 start\n",
      "25285 stori\n",
      "68322 t\n",
      "151030 thi\n",
      "16512 thing\n",
      "17546 think\n",
      "7604 thought\n",
      "31968 time\n",
      "12535 tri\n",
      "7623 turn\n",
      "10228 use\n",
      "10244 ve\n",
      "27729 veri\n",
      "95626 wa\n",
      "13180 want\n",
      "27882 watch\n",
      "17235 way\n",
      "10507 whi\n",
      "13894 work\n",
      "7739 world\n",
      "13249 year\n"
     ]
    }
   ],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(corpus_data_features_nd, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bag-of-words linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform logistic regression in Python we use [sklearn.linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). But first let's split our training data in order to get an evaluation set. We will use [sklearn.cross_validation.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# remember that corpus_data_features_nd contains all of our original train and test data, so we need to exclude\n",
    "# the unlabeled test entries\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "    corpus_data_features_nd[0:len(train_data_df)], \n",
    "    train_data_df.Sentiment,\n",
    "    train_size=0.85, \n",
    "    random_state=1234)\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the classifier to label our evaluation set. We can use either `predict` for classes or `predict_proba` for probabilities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a function for classification called [sklearn.metrics.classification_report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) which calculates several types of (predictive) scores on a classification model. Check also [sklearn.metrics](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics). In this case we want to check our classifier accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.87      0.86      0.87      1920\n",
      "   POSITIVE       0.85      0.87      0.86      1830\n",
      "\n",
      "avg / total       0.86      0.86      0.86      3750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can re-train our model with all the training data and use it for sentiment classification with the original (unlabeled) test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.86      0.85      0.86     12584\n",
      "   POSITIVE       0.85      0.86      0.86     12416\n",
      "\n",
      "avg / total       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train classifier\n",
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=corpus_data_features_nd[0:len(train_data_df)], y=train_data_df.Sentiment)\n",
    "\n",
    "# get predictions\n",
    "test_pred = log_model.predict(corpus_data_features_nd[len(train_data_df):])\n",
    "actual_pred = test_data_df[\"Sentiment\"].tolist()\n",
    "\n",
    "print(classification_report(test_pred, actual_pred))\n",
    "\n",
    "# sample some of them\n",
    "\n",
    "# spl = range(len(test_pred))\n",
    "\n",
    "# target = open(\"logistic_output.txt\", 'w')\n",
    "\n",
    "# # print text and labels\n",
    "# for text, sentiment in zip(test_data_df.Text[spl], test_pred[spl]):\n",
    "#     target.write(sentiment + \"|\" + text + \"\\n\")\n",
    "# target.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
